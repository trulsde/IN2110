{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN2110 obligatorisk innlevering 2b\n",
    "\n",
    "Oppgaven inneholder to deler (maskinoversettelse og interaktive systemer). Planlegg god tid slik at du klarer å fullføre de to delene av oppgaven.\n",
    "\n",
    "Dersom du har spørsmål så kan du:\n",
    "\n",
    "* gå på gruppetime,\n",
    "* spørre på Discourse (https://in2110-discourse.uio.no/)\n",
    "* eller sende epost til in2110-hjelp@ifi.uio.no dersom alternativene over av en eller annen grunn ikke passer for spørsmålet ditt.\n",
    "\n",
    "### Oppsett\n",
    "Når du har klonet dette github-repoet som denne notebooken ligger i, har du tilgang til datene som ligger i denne mappa. Hvis du ønsker å kopiere denne mappa, \"2b\", over til et annet sted, så skulle det gå bra. Bare pass på at du følger med på om det er oppdateringer her i repoet som gir ut obligen. Du bør også installere `transformers`, `pytorch` og `sentencepiece`:\n",
    "```bash\n",
    "> pip install transformers \"transformers[sentencepiece]\" torch\n",
    "```\n",
    "\n",
    "### Innlevering\n",
    "\n",
    "Innleveringen skal bestå av: \n",
    "- denne Jupyter notebook fylt ut med både kode og tilhørende forklaringer.\n",
    "- de to opprinnelige tekstfilene `lotr.de` og `lotr.en` som kom med obligen\n",
    "- de to tekstfilene `lotr_output.en` og `lotr_corrected_en` som ble generert i Del 1 \n",
    " \n",
    "Vi understreker at innlevering av koden alene __ikke er nok__ for å bestå oppgaven -- vi forventer at notebooken også skal inneholde beskrivelser (på norsk eller engelsk) av hva dere har gjort og begrunnelser for valgene dere har tatt underveis. Bruk helst hele setninger, og matematiske formler om nødvendig. Evalueringstallene bør presenteres i tabeller. Det å  forklare med egne ord (samt begreper vi har gått gjennom på forelesningene) hva dere har implementert og reflektere over hvorvidt løsningen dere har lagt  besvarer oppgaven er en viktig del av læringsprosessen -- ta det på alvor! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Del 1: Maskinoversettelse\n",
    "\n",
    "Vi skal bruke en (nevral) maskinoversettelsemodell til å oversette filmtekstinger fra Ringenes Herre (og Hobbiten) fra tysk til engelsk.\n",
    "\n",
    "## Data\n",
    "\n",
    "Filmtekstingene ligger i filene `lotr.de` og `lotr.en` for henholdsvis de tyske og engelskspråklige filmtekstingene. Disse to filene utgjør et såkalt _parallellkorpus_, altså en tekstsamling hvor hver setning (i språk A) er koblet til en tilsvarende setning i språk B. De 2 filene har samme antall linjer, slik at den tyske setningen på linjen $i$ av `lotr.de` har en engelsk oversettelse på samme linje av `lotr.en`. Filmtekstingene er ekstrahert fra korpuset [OpenSubtitles-2018](http://opus.nlpl.eu/OpenSubtitles-v2018.php).\n",
    "\n",
    "\n",
    "Her er f.eks. de 10 første linjene i `lotr.de` og `lotr.en`: \n",
    "<style scoped>\n",
    "table {\n",
    "  font-size: 12px;\n",
    "}\n",
    "</style>\n",
    "|   | Tysk (`lotr.de`)         | Engelsk (`lotr.en`)      |\n",
    "|---|--------------------------|--------------------------|\n",
    "| 1 | Die Welt ist im Wandel . | The world is changed .   |\n",
    "| 2 | Ich spüre es im Wasser . | I feel it in the water . |\n",
    "| 3 | Ich spüre es in der Erde . | I feel it in the earth . |\n",
    "| 4 | Ich rieche es in der Luft . | I smell it in the air . |\n",
    "| 5 | Vieles , was einst war , ist verloren , da niemand mehr lebt , der sich erinnert . | Much that once was is lost . For none now live who remember it . |\n",
    "| 6 | Es begann mit dem Schmieden der Großen Ringe . | It began with the forging of the Great Rings . |\n",
    "| 7 | 3 wurden den Elben gegeben , den unsterblichen , weisesten und reinsten aller Wesen . | Three were given to the Elves : Immortal , wisest and fairest of all beings . |\n",
    "| 8 | 7 den Zwergenherrschern , großen Bergleuten und Handwerkern in ihren Hallen aus Stein . | Seven to the Dwarf-lords : Great miners and craftsmen of the mountain halls . |\n",
    "| 9 | Und 9 ... 9 Ringe wurden den Menschen geschenkt , die vor allem anderen nach Macht streben . | And nine nine rings were gifted to the race of Men who , above all else , desire power . |\n",
    "| 10 | Denn diese Ringe bargen die Kraft und den Willen , jedes Volk zu leiten . | For within these rings was bound the strength and will to govern each race . |\n",
    "\n",
    "Merk at teksten allerede er tokenisert. Noen ganger kan det være store sprik mellom innholdet i filmtekstingene. Det er ikke nødvendigvis en oversettelsefeil -- det er bare at filmtekstere kan velge å transkribere hva som skjer i filmen på litt ulike måter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Komme i gang\n",
    "\n",
    "Vi skal bruke [`transformers`](https://huggingface.co/docs/transformers/index), et Python-bibliotek fra HuggingFace som gjør det lettere å kjøre nevrale modeller i NLP, blant annet for maskinoversettelse. Vi skal imidlertid ikke trene en ny modell, da trening av slike modeller krever ganske store regneressurser, blant annet GPUs. Men heldigsvis finnes det allerede pre-trente modeller. \n",
    "\n",
    "Vi skal benytte oss av [`opus-mt-de-en`](https://huggingface.co/Helsinki-NLP/opus-mt-de-en), som består av en _tokenizer_ og selve _seq2seq modellen_ (som baserer seg på [MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)). Disse to kan lastes slik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: torch in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp39-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from transformers[sentencepiece]) (5.26.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kroel\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kroel\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kroel\\appdata\\local\\miniconda3\\envs\\in2110_v24\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.1\n",
      "    Uninstalling tokenizers-0.15.1:\n",
      "      Successfully uninstalled tokenizers-0.15.1\n",
      "Successfully installed tokenizers-0.19.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers \"transformers[sentencepiece]\" torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"helsinki-nlp/opus-mt-de-en\", force_downgrade=True)\n",
    "translator = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"helsinki-nlp/opus-mt-de-en\")\n",
    "\n",
    "# Hvis du har en GPU på maskinen din kan du sette device til \"cuda\" \n",
    "# i stedet for \"cpu\", slik at oversettelsen går raskere\n",
    "device = \"cpu\"  # \"cuda\"\n",
    "translator = translator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "translator = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"helsinki-nlp/opus-mt-de-en\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når dere kjører disse to linjene for første gang vil biblioteket automatisk laste ned modellen fra HuggingFace sin repo.\n",
    "\n",
    "Den _tokenizer_ har ansvar for for å segmentere input-strengene i tokens og konvertere disse til tall (indeksverdier fra modellens vokabular). Den kan brukes slik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  55,  401,   29,   49, 9012,    3,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Die Welt ist im Wandel.\", return_tensors=\"pt\", padding=True)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argumentet `return_tensors='pt'` brukes til å få Pytorch tensorer som resultater i stedet for vanlige lister. Merk også at vi aktiverer padding til å få de to sekvensene til å ha samme lengde, slik at de to tokensekvensene kan settes sammen i én tensor. Resultatet inkluderer både selve listen over `token IDs` og en `attention mask`. Sistnevnte består vanligvis av rekker av 1 (det vil si at modellen må ta hensyn til alle tokens) bortsett fra de kunstige \"padding\"-tokens som er lagt til for å få sekvenser av samme lengde. \n",
    "\n",
    "Deretter er det bare å bruke funksjonen `generate` til å kjøre oversettelsen (vi setter her en maks grense på 50 tokens for hver oversettelse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[58100,    36,   360,    19,  7315,     3,     0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = {k:v.to(device) for k, v in tokens.items()}  # Flytt data til riktig enhet (i tilfelle man bruker GPU)\n",
    "outputs = translator.generate(**tokens, max_new_tokens=50)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultatet er igjen en liste over token IDs, slik at vi må konvertere disse tilbake til strenger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is changing.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations =tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "translations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er selsvagt mange parametre dere kan eksperimentere hvis dere ønsker å tilpasse oversettelsestrategien (se [her](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)), men det er egentlig alt dere trenger å vite for å bruke modellen til å oversette!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversettelser\n",
    "\n",
    "__Oppgave 1.1__: Finn ut hvordan `tokenizer` har delt opp de to setningene \"Die Welt ist im Wandel.\" og \"Ich spüre es im Wasser.\" i en rekke av tokens, og vis inndelingen. Hvis du ser at `tokenizer` har lagt til spesielle symboler i de 2 rekkene bør du også forklare kort hva disse står for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Svar: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Oppgave 1.2__ : Bruk `opus-mt-de-en` modellen vist over til å oversette hele korpuset `lotr.de` til engelsk. Merk at du trolig må kjøre dele oversettelsen i flere _batch_ for å unngå at Python krasjer hvis minnen ikke strekker til. \n",
    "\n",
    "Oversettelsen kan ta litt tid avhengig av maskinen deres (og spesielt om maskinen har tilgang til en GPU). Hvis du opplever store utfordringer med regnekraft kan du redusere antall linjer på filene `lotr.de` og `lotr.en`, slik at det blir mindre å oversette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeg tok sjansen på å kjøre denne koden i ett sett - på tross av at PCen min ikke har GPU. Det tok en time, men gikk fint!\n",
    "import re\n",
    "\n",
    "def translate(input_file, translation_file):\n",
    "    \"\"\"Translate an input file line by line using the opus-mt-de-en model,\n",
    "    and write the translations to output_file. The two files should\n",
    "    have the same number of lines\"\"\"\n",
    "\n",
    "    with open(input_file, 'r') as input, open(translation_file, 'w') as output:\n",
    "        count = 0\n",
    "        for line in input:\n",
    "            tokens = tokenizer(line, return_tensors=\"pt\", padding=True)\n",
    "            tokens = {k:v.to(device) for k, v in tokens.items()}\n",
    "            outputs = translator.generate(**tokens, max_new_tokens=50)\n",
    "            translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            translation = re.sub(r'(?<=\\w)([,.;.])', r' \\1', translation)\n",
    "\n",
    "            output.write(f'{translation}\\n')\n",
    "            count += 1\n",
    "\n",
    "translate(\"lotr.de\", \"lotr_output.en\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dere kommer sikkert til å merke at modellen ikke ble trent på tekster fra Ringenes Herre og dermed ikke oversetter navnene på personer og steder på en god måte. For eksempel er `Bilbo Baggins` egentlig oversatt til `Bilbo Beutlin` på tysk, og `the Shire` er oversatt til `Auenland`. Men det er oversettelsesystemet vårt uvitende om. \n",
    "\n",
    "__Oppgave 1.3__: Lag en postprosesseringsmetode som redigerer feile oversettelser for en rekke navn fra Ringenes Herre som er annerledes på tysk og på engelsk. Det er nok om du finner 10 navn som bør rettes opp, men du kan selvsagt legge til flere om du ønsker det."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(translation_file, new_translation_file):\n",
    "    \"\"\"Edits some of the translations in translation_file to correct\n",
    "    some erroneous translations due to Lord of the Rings names.\"\"\"\n",
    "\n",
    "    name_edits = {(\"Destiny Hill\", \"Fate Hill\", \"Fatesberg\"): \"Mount Doom\", \"Dark Ruler\": \"Dark Lord\",\n",
    "                   (\"foggy mountains\", \"Fog mountains\", \"Fog Mountains\"): \"misty mountains\", (\"darling\", \"Honey\", \"treasure\"): \"precious\", \"Auenland\": \"Shire\",\n",
    "                   (\"beutlin\", \"Beutlin\"): \"Baggins\", \"Tobi\": \"Toby\", \"Tuk\": \"Took\", \"Hobbingen\": \"Hobbington\",\n",
    "                   (\"Elbish\", \"Elbian\"): \"Elvish\", \"Samweis\": \"Samwise\", \"Gamjie\": \"Gamgee\", \"Isengart\": \"Isengard\",\n",
    "                   (\"dance pony\", \"dancing pony\"): \"Prancing Pony\", \"Unterberg\": \"Underhill\", \"Bruchtal\": \"Rivendell\",\n",
    "                   (\"Stich\"): \"Sting\", (\"Eisenbergen\"): \"Iron Hills\", \"Hüttinger\": \"Cotton\", \"Bauer\": \"Farmer\", \"Black Horseman\": \"Black Rider\",\n",
    "                   \"Herr\": \"Mr.\"}\n",
    "\n",
    "    with open(translation_file, 'r') as input, open(new_translation_file, 'w') as output:\n",
    "        for line in input:\n",
    "            for key in name_edits.keys():\n",
    "                if isinstance(key, tuple):\n",
    "                    for name in key:\n",
    "                        if name in line:\n",
    "                            line = line.replace(name, name_edits[key])\n",
    "                else:\n",
    "                    if key in line:\n",
    "                        line = line.replace(key, name_edits[key])\n",
    "            output.write(line)\n",
    "\n",
    "        \n",
    "\n",
    "postprocess(\"lotr_output.en\", \"lotr_corrected.en\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluering\n",
    "\n",
    "Vi er nå klare til å evaluere kvaliteten på oversettelsene vi har generert. Her skal vi bruke en evalueringsmetode som er veldig populær i maskinoversettelse, nemlig __BLEU__.  BLEU er en automatisert evalueringsmetode som sammenligner oversettelse som systemet har produsert med en eller flere fasiter, altså oversettelser skrevet av menneskelige eksperter.  I vårt tilfelle er fasiten de engelskspråklige filmtekstingene i `lotr.en`. \n",
    "\n",
    "BLEU beregnes ved å se på _overlapp_ mellom N-grams fra fasiten(e) og oversettelsene fra systemet. Mer presist ekstraherer vi for hver setning alle N-grams (med N fra 1 til 4) fra både systemet og fasiten, og beregner hva som er precision for $i \\in {1,2,3,4}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    precision_i = \\frac{\\text{Antall $i$-grams som forekommer i både system og fasit (for samme setning)}}{\\text{Antall $i$-grams i setningene fra systemet}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Deretter slår vi sammen precision-tallene:\n",
    "\n",
    "\\begin{equation}\n",
    "BLEU = brevity\\_penalty * \\left(\\prod_{i=1}^4 precision_i \\right)^{\\frac{1}{4}}\n",
    "\\end{equation}\n",
    "\n",
    "hvor \"brevity penalty\" brukes til å straffe modeller som produserer for korte oversettelser:\n",
    "\n",
    "\\begin{equation}brevity\\_penalty = min(1, \\frac{\\text{Antall ord i systemets setninger}}{\\text{Antall ord i fasitens setninger}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kode\n",
    "\n",
    "Her er del av koden som kan brukes til å beregne BLEU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def get_sentences(text_file):\n",
    "    \"\"\"Given a text file with one (tokenised) sentence per line, returns a list \n",
    "    of sentences , where each sentence is itself represented as a list of tokens.\n",
    "    The tokens are all converted into lowercase.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    fd = open(text_file)\n",
    "    for sentence_line in fd:\n",
    "        # We convert everything to lowercase\n",
    "        sentence_line = sentence_line.rstrip(\"\\n\").lower()\n",
    "        sentences.append(sentence_line.split())\n",
    "    fd.close()\n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "\n",
    "def get_ngrams(tokens, ngram_order):\n",
    "    \"\"\"\n",
    "    Extracts all n-grams counts of a given order from an input sequence of tokens.\n",
    "    \"\"\"\n",
    "    ngrams = collections.Counter()\n",
    "    for i in range(0, len(tokens) - ngram_order + 1):\n",
    "        ngram = tuple(tokens[i:i+ngram_order])\n",
    "        ngrams[ngram] += 1\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def compute_brevity_penalty(reference_file, output_file):\n",
    "    \"\"\"Computes the brevity penalty.\"\"\"\n",
    "    \n",
    "    ref_sentences = get_sentences(reference_file)\n",
    "    output_sentences = get_sentences(output_file)\n",
    "    \n",
    "    nb_ref_tokens = sum([len(sentence) for sentence in ref_sentences])\n",
    "    nb_output_tokens = sum([len(sentence) for sentence in output_sentences])\n",
    "    \n",
    "    penalty = min(1, nb_output_tokens/nb_ref_tokens)\n",
    "    return penalty\n",
    "\n",
    "    \n",
    "def compute_bleu(reference_file, output_file, max_order=4):\n",
    "    \"\"\"\n",
    "    Given a reference file, an output file from the translation system, and a \n",
    "    maximum order for the N-grams, computes the BLEU score for the translations \n",
    "    in the output file.\n",
    "    \"\"\"\n",
    "   \n",
    "    precision_product = 1\n",
    "    for i in range(1, max_order+1):\n",
    "        precision_product *= compute_precision(reference_file, output_file, i) \n",
    "    \n",
    "    brevity_penalty = compute_brevity_penalty(reference_file, output_file)\n",
    "    \n",
    "    bleu = brevity_penalty * math.pow(precision_product, 1/max_order)\n",
    "    return bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'world', 'is', 'changing', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentences(\"lotr_corrected.en\")[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Oppgave 1.4__: Koden over mangler funksjonen `compute_precision(ref_file, output_file, ngram_order)` som beregner _precision_-verdien (som definert over) for en gitt N-gram ordre. Implementer denne metode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6103174603174604\n"
     ]
    }
   ],
   "source": [
    "def compute_precision(reference_file, output_file, ngram_order):\n",
    "    \"\"\"\n",
    "    Computes the precision score for a given N-gram order. The first file contains the \n",
    "    reference translations, while the second file contains the translations actually\n",
    "    produced by the system. ngram_order is 1 to compute the precision over unigrams, \n",
    "    2 for the precision over bigrams, and so forth.   \n",
    "    \"\"\"\n",
    "    \n",
    "    ref_sentences = get_sentences(reference_file)\n",
    "    output_sentences = get_sentences(output_file)\n",
    "\n",
    "    ngrams_match = 0\n",
    "    ngrams_total = 0\n",
    "    \n",
    "    for sen1, sen2 in zip(ref_sentences, output_sentences):\n",
    "        ref_grams = get_ngrams(sen1, ngram_order)\n",
    "        out_grams = get_ngrams(sen2, ngram_order)\n",
    "        for key in out_grams.keys():\n",
    "            if key in ref_grams.keys():\n",
    "                ngrams_match += ref_grams[key]\n",
    "            ngrams_total += out_grams[key]\n",
    "    \n",
    "    return ngrams_match / ngrams_total\n",
    "\n",
    "print(compute_precision(\"lotr.en\", \"lotr_output.en\", 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tips_: bruk metoden `get_ngrams(tokens, ngram_order)` som ekstraherer alle N-grams i en setning (allerede inndelt i tokens), og som allerede er implementert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Oppgave 1.5__: Kjør funksjonen `compute_bleu(ref_file, output_file)` på oversettelsene du har produsert. Sammenlign BLEU-resultatet du får både _med_ og _uten_ å ta i bruk postprocesseringsmetoden du har skrevet i oppgave 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2818837317777583\n",
      "0.2776774042360547\n"
     ]
    }
   ],
   "source": [
    "print(compute_bleu(\"lotr.en\", \"lotr_corrected.en\"))\n",
    "print(compute_bleu(\"lotr.en\", \"lotr_output.en\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del 2: Interaktive systemer\n",
    "\n",
    "Filmtekstinger kan brukes til andre formål enn å trene og teste maskinoversettelsesystemer - filmtekstinger består også i all hovedsak av _samtaler_ og kan derfor også brukes til å bygge datadrevne dialogsystemer! \n",
    "\n",
    "I denne delen av oppgaven skal dere utvikle en liten _retrieval-based chatbot_ basert på korpuset i `lotr.en`. Chatboten vår vil derfor \"snakke\" som filmkarakterer i Ringene Herre. Hovedidéen er å:\n",
    "* Beregne TF-IDF-vektorene av alle setningene i korpuset vårt og lagre disse. La oss kalle disse vektorene $[t_1, t_2, ... t_{|C|}]$, hvor $|C|$ er antall setninger i korpuset. \n",
    "* Når chatbot mottar en ny inputsetning fra brukeren beregner vi TF-IDF vektoren $q$ av denne setningen.\n",
    "* Deretter leter man etter setningen i korpuset som ligner mest på inputsetningen ved å beregne _cosine similarity_ mellom TF-IDF vektoren $q$ av inputsetningen og hver TF-IDF-vektor fra korpuset $C$:\n",
    "\n",
    "    \\begin{equation}\n",
    "        i^* = \\argmax_{i=1}^{|C|} \\frac{q^T t_i}{||q|| \\ ||t_i||}\n",
    "    \\end{equation}\n",
    "\n",
    "* Til slutt tar vi setningen som kommer _rett etter_ setningen $t_{i^*}$, altså $t_{i^*+1}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksempel\n",
    "\n",
    "For å ta et eksempel: la oss si at brukeren skriver: \n",
    "\n",
    "````{verbatim}\n",
    "Are you Bilbo Baggins ?\n",
    "````\n",
    "\n",
    "Ifølge cosine similarity mellom TF-IDF vektorer er setningen i korpuset som ligner mest på inputsetningen på linje 4907:\n",
    "\n",
    "````{verbatim}\n",
    "Bilbo Baggins .\n",
    "````\n",
    "\n",
    "Da vil systemet ta setningen på linjen 4908 og svare brukeren:\n",
    "\n",
    "````{verbatim}\n",
    "I 'm sorry , do I know you ?\n",
    "````\n",
    "\n",
    "### Kode\n",
    "\n",
    "Her er en del av koden som skal brukes i vår chatbot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalChatbot:\n",
    "    \"\"\"Retrieval-based chatbot using TF-IDF vectors\"\"\"\n",
    "    \n",
    "    def __init__(self, dialogue_file):\n",
    "        \"\"\"Given a corpus of dialoge utterances (one per line), computes the\n",
    "        document frequencies and TF-IDF vectors for each utterance\"\"\"\n",
    "\n",
    "        # I also store the dialogue file as an instance variable, to generate outputs directly from here\n",
    "        self.dialogue_file = dialogue_file\n",
    "        \n",
    "        # We store all utterances (as lists of lowercased tokens)\n",
    "        self.utterances = []\n",
    "        fd = open(dialogue_file)\n",
    "        for line in fd:\n",
    "            utterance = self._tokenise(line.rstrip(\"\\n\"))\n",
    "            self.utterances.append(utterance)\n",
    "        fd.close()\n",
    "        \n",
    "        self.doc_freqs = self._compute_doc_frequencies()\n",
    "        self.tf_idfs = [self.get_tf_idf(utterance) for utterance in self.utterances]\n",
    "\n",
    "        \n",
    "    def _tokenise(self, utterance):\n",
    "        \"\"\"Convert an utterance to lowercase and tokenise it by splitting on space\"\"\"\n",
    "        return utterance.strip().lower().split()\n",
    "    \n",
    "    def _compute_doc_frequencies(self):\n",
    "        \"\"\"Compute the document frequencies (necessary for IDF)\"\"\"\n",
    "        \n",
    "        doc_freqs = {}\n",
    "        for utterance in self.utterances:\n",
    "            for word in set(utterance):\n",
    "                doc_freqs[word] = doc_freqs.get(word, 0) + 1\n",
    "        return doc_freqs\n",
    "\n",
    "    \n",
    "    def get_tf_idf(self, utterance):\n",
    "        \"\"\"Compute the TF-IDF vector of an utterance. The vector can be represented \n",
    "        as a dictionary mapping words to TF-IDF scores.\"\"\"\n",
    "         \n",
    "        tf_idf_vals = {}\n",
    "        word_counts = {word:utterance.count(word) for word in utterance}\n",
    "        for word, count in word_counts.items():\n",
    "            idf = math.log(len(self.utterances)/(self.doc_freqs.get(word,0) + 1))\n",
    "            tf_idf_vals[word] = count * idf\n",
    "        return tf_idf_vals\n",
    "    \n",
    "    \n",
    "    def get_response(self, query):\n",
    "        \"\"\"\n",
    "        Finds out the utterance in the corpus that is closed to the query\n",
    "        (based on cosine similarity with TF-IDF vectors) and returns the \n",
    "        utterance following it. \n",
    "        \"\"\"\n",
    "\n",
    "        # If the query is a string, we first tokenise it\n",
    "        if type(query)==str:\n",
    "            query = self._tokenise(query)\n",
    "        \n",
    "        # Your implementation should use the get_tf_idf and compute_cosine \n",
    "        # methods that are already provided (as well as the TF-IDF values\n",
    "        # from each utterance in the corpus, stored in self.tf_idfs) \n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    \n",
    "    def compute_cosine(self, tf_idf1, tf_idf2):\n",
    "        \"\"\"Computes the cosine similarity between two vectors\"\"\"\n",
    "        \n",
    "        dotproduct = 0\n",
    "        for word, tf_idf_val in tf_idf1.items():\n",
    "            if word in tf_idf2:\n",
    "                dotproduct += tf_idf_val*tf_idf2[word]\n",
    "                \n",
    "        return dotproduct / (self._get_norm(tf_idf1) * self._get_norm(tf_idf2))\n",
    "    \n",
    "    def _get_norm(self, tf_idf):\n",
    "        \"\"\"Compute the vector norm\"\"\"\n",
    "        \n",
    "        return math.sqrt(sum([v**2 for v in tf_idf.values()]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Oppgave 2.1__: Fyll ut metoden get_response(self, query) som tar en brukersetning som input, og returnerer svaret som forklart ovenfor. Metoden bør ta i bruk de to metodene `get_tf_idf` og `compute_cosine` som allerede er implementert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(self, query):\n",
    "    \"\"\"\n",
    "    Finds out the utterance in the corpus that is closed to the query\n",
    "    (based on cosine similarity with TF-IDF vectors) and returns the \n",
    "    utterance following it. \n",
    "    \"\"\"\n",
    "\n",
    "    # If the query is a string, we first tokenise it\n",
    "    answer = \"Something went wrong!\"\n",
    "\n",
    "    if type(query)==str:\n",
    "\n",
    "        answer_index = 0\n",
    "        best_cosine_similarity = -1\n",
    "\n",
    "        query = self._tokenise(query)\n",
    "        query_idf = self.get_tf_idf(query)\n",
    "        for tf_idf in self.tf_idfs:\n",
    "            sim = abs(self.compute_cosine(tf_idf, query_idf))\n",
    "            if sim > best_cosine_similarity:\n",
    "                best_cosine_similarity = sim\n",
    "                answer_index = self.tf_idfs.index(tf_idf) + 1\n",
    "        \n",
    "        with open(self.dialogue_file) as dialogue:\n",
    "            answer = dialogue.readlines()[answer_index]\n",
    "        \n",
    "        return answer.strip()\n",
    "        \n",
    "        \n",
    "    # Your implementation should use the get_tf_idf and compute_cosine \n",
    "    # methods that are already provided (as well as the TF-IDF values\n",
    "    # from each utterance in the corpus, stored in self.tf_idfs)\n",
    "\n",
    "    # Implementer metoden her!\n",
    "\n",
    "RetrievalChatbot.get_response = get_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I 'm sorry , do I know you ?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_of_the_rings = RetrievalChatbot(\"lotr.en\")\n",
    "word_of_the_rings.get_response(\"Bilbo Baggins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan deretter teste din chatbot med ulike brukerinput og se hva som kommer ut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fly , you fools !'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_of_the_rings.get_response(\"You shall not pass!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One does not simply walk into Mordor .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_of_the_rings.get_response(\"One of you must do this\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "3529d1d89cb4c8d13e402e4117b8dc865480f261d07f0716e8c60a591b54d3ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
